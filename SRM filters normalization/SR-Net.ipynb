{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SR-Net Image and SRM Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage, signal\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import ntpath\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from keras import optimizers \n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from time import time\n",
    "import time as tm\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "import glob\n",
    "from skimage.util.shape import view_as_blocks\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Concatenate, Lambda,Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization, ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 SRM filters for preprocessing and the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('SRM_Kernels.npy')*(1/12) \n",
    "biasSRM=numpy.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR_Net(img_size=256):\n",
    "    tf.keras.backend.clear_session()\n",
    "    inputs = tf.keras.Input(shape=(img_size,img_size,1), name=\"input\")\n",
    "    # Layer 1\n",
    "    conv0 =   tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "    conv1 =   tf.keras.layers.Conv2D(64, (3,3), strides=(1,1), activation=\"relu\", kernel_initializer='glorot_normal')(conv0)\n",
    "    bn1 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv1)\n",
    "    # Layer 2\n",
    "    conv2 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", kernel_initializer='glorot_normal')(bn1)\n",
    "    bn2 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv2)\n",
    "    # Layer 3\n",
    "    conv3 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn2)\n",
    "    bn3 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv3)\n",
    "    conv4 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn3)\n",
    "    bn4 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv4)\n",
    "    skip1 =   tf.keras.layers.Add()([bn2, bn4])\n",
    "    # Layer 4\n",
    "    conv5 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip1)\n",
    "    bn5 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv5)\n",
    "    conv6 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn5)\n",
    "    bn6 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv6)\n",
    "    skip2 =   tf.keras.layers.Add()([skip1, bn6])\n",
    "    # Layer 5\n",
    "    conv7 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip2)\n",
    "    bn7 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv7)\n",
    "    conv8 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn7)\n",
    "    bn8 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv8)\n",
    "    skip3 =   tf.keras.layers.Add()([skip2, bn8])\n",
    "    # Layer 6\n",
    "    conv9 =   tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip3)\n",
    "    bn9 =     tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv9)\n",
    "    conv10 =  tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn9)\n",
    "    bn10 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv10)\n",
    "    skip4 =   tf.keras.layers.Add()([skip3, bn10])\n",
    "    # Layer 7\n",
    "    conv11 =  tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip4)\n",
    "    bn11 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv11)\n",
    "    conv12 =  tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn11)\n",
    "    bn12 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv12)\n",
    "    skip5 =   tf.keras.layers.Add()([skip4, bn12])\n",
    "    # Layer 8\n",
    "    conv13 =  tf.keras.layers.Conv2D(16, (1,1), strides=(2,2), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip5)\n",
    "    bn13 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv13)\n",
    "    conv14 =  tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip5)\n",
    "    bn14 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv14)\n",
    "    conv15 =  tf.keras.layers.Conv2D(16, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn14)\n",
    "    bn15 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv15)\n",
    "    pool1 =   tf.keras.layers.AveragePooling2D([3,3], (2,2), 'SAME', data_format='channels_last')(bn15)\n",
    "    skip6 =   tf.keras.layers.Add()([bn13, pool1])\n",
    "    # Layer 9\n",
    "    conv16 =  tf.keras.layers.Conv2D(64, (1,1), strides=(2,2), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip6)\n",
    "    bn16 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv16)\n",
    "    conv17 =  tf.keras.layers.Conv2D(63, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip6)\n",
    "    bn17 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv17)\n",
    "    conv18 =  tf.keras.layers.Conv2D(64, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn17)\n",
    "    bn18 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv18)\n",
    "    pool2 =   tf.keras.layers.AveragePooling2D([3,3], (2,2), 'SAME', data_format='channels_last')(bn18)\n",
    "    skip7 =   tf.keras.layers.Add()([bn16, pool2])\n",
    "    # Layer 10\n",
    "    conv19 =  tf.keras.layers.Conv2D(128, (1,1), strides=(2,2), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip7)\n",
    "    bn19 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv19)\n",
    "    conv20 =  tf.keras.layers.Conv2D(128, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip7)\n",
    "    bn20 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv20)\n",
    "    conv21 =  tf.keras.layers.Conv2D(128, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn20)\n",
    "    bn21 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv21)\n",
    "    pool3 =   tf.keras.layers.AveragePooling2D([3,3], (2,2), 'SAME', data_format='channels_last')(bn21)\n",
    "    skip8 =   tf.keras.layers.Add()([bn19, pool3])\n",
    "    # Layer 11\n",
    "    conv22 =  tf.keras.layers.Conv2D(256, (1,1), strides=(2,2), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip8)\n",
    "    bn22 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv22)\n",
    "    conv23 =  tf.keras.layers.Conv2D(256, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip8)\n",
    "    bn23 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv23)\n",
    "    conv24 =  tf.keras.layers.Conv2D(256, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn23)\n",
    "    bn24 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv24)\n",
    "    pool4 =   tf.keras.layers.AveragePooling2D([3,3], (2,2), 'SAME', data_format='channels_last')(bn24)\n",
    "    skip9 =   tf.keras.layers.Add()([bn22, pool4])\n",
    "    # Layer 12\n",
    "    conv25 =  tf.keras.layers.Conv2D(512, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(skip9)\n",
    "    bn25 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv25)\n",
    "    conv26 =  tf.keras.layers.Conv2D(512, (3,3), strides=(1,1), activation=\"relu\", padding='SAME', kernel_initializer='glorot_normal')(bn25)\n",
    "    bn26 =    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(conv26)\n",
    "    GAP =     tf.keras.layers.GlobalAveragePooling2D(data_format = 'channels_last')(bn26)\n",
    "    #Layer 13, FC, Softmax\n",
    "    layers1 = tf.keras.layers.Dense(128,activation=\"relu\")(GAP)\n",
    "    layers1 = tf.keras.layers.Dropout(0.2)(layers1)\n",
    "    layers1 = tf.keras.layers.Dense(64 ,activation=\"relu\")(layers1)\n",
    "    layers1 = tf.keras.layers.Dropout(0.2)(layers1)\n",
    "    layers1 = tf.keras.layers.Dense(32 ,activation=\"relu\")(layers1)\n",
    "    \n",
    "    #Softmax\n",
    "    predictions = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"output\")(layers1)\n",
    "    \n",
    "    #Model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer=tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9) \n",
    "    \n",
    "    #Compilator\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print (\"SR-net model generated\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining different functions to work with the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, model_name=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=path_log_base+\"/\"+model_name+\"_\"+str(datetime.datetime.now().isoformat()[:19].replace(\"T\", \"_\").replace(\":\",\"-\"))\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    filepath = log_dir+\"/saved-model-{epoch:03d}-{val_accuracy:.4f}.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max')\n",
    "    model.reset_states()\n",
    "    \n",
    "    #VALORES EN TRAIN TEST Y VALIDACIÓN INICIALES, GRÁFICOS\n",
    "    global lossTEST\n",
    "    global accuracyTEST\n",
    "    global lossTRAIN\n",
    "    global accuracyTRAIN\n",
    "    global lossVALID\n",
    "    global accuracyVALID\n",
    "    lossTEST,accuracyTEST   = model.evaluate(X_test, y_test,verbose=None)\n",
    "    lossTRAIN,accuracyTRAIN = model.evaluate(X_train, y_train,verbose=None)\n",
    "    lossVALID,accuracyVALID = model.evaluate(X_valid, y_valid,verbose=None)\n",
    "\n",
    "    global history\n",
    "    global model_Name\n",
    "    global log_Dir\n",
    "    model_Name = model_name\n",
    "    log_Dir = log_dir\n",
    "    \n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                      callbacks=[tensorboard,checkpoint], \n",
    "                      batch_size=batch_size,validation_data=(X_valid, y_valid),verbose=1)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "     \n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(log_dir)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Test(model,PATH_trained_models):\n",
    "    global AccTest\n",
    "    global LossTest\n",
    "    AccTest = []\n",
    "    LossTest= [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "      if filename != ('train') and filename != ('validation'):\n",
    "        print(filename)\n",
    "        model.load_weights(PATH_trained_models+'/'+filename)\n",
    "        loss,accuracy = model.evaluate(X_test, y_test,verbose=0)\n",
    "        print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n') \n",
    "        BandAccTest  = accuracy\n",
    "        BandLossTest = loss\n",
    "        AccTest.append(BandAccTest)    #Valores de la precisión en Test, para graficar junto a valid y train\n",
    "        LossTest.append(BandLossTest)  #Valores de la perdida en Test, para graficar junto a valid y train\n",
    "        if accuracy > B_accuracy:\n",
    "          B_accuracy = accuracy\n",
    "          B_loss = loss\n",
    "          B_name = filename\n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID):\n",
    "    numbers=AccTest\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Test Accuracy {}, Época:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['accuracy']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Train Accuracy {}, Época:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['val_accuracy']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Validation Accuracy {}, Época:{}\\n\".format(value, index+1))\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(history.history['accuracy'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyVALID]),np.array(history.history['val_accuracy'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTEST]),np.array(AccTest)],axis=0)) #Test\n",
    "        plt.title('Accuracy Vs Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/Accuracy_SR_Net_'+model_Name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/Accuracy_SR_Net_'+model_Name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/Accuracy_SR_Net_'+model_Name+'.pdf', format='pdf')     \n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(history.history['loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossVALID]),np.array(history.history['val_loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossTEST]),np.array(LossTest)],axis=0)) #Test\n",
    "        plt.title('Loss Vs Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/Loss_SR_Net_'+model_Name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/Loss_SR_Net_'+model_Name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/Loss_SR_Net_'+model_Name+'.pdf', format='pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_models(AccTest,AccTrain,AccValid):\n",
    "    numbers=AccTest\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Test Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=AccTrain\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Train Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=AccValid\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Validation Accuracy {}, epoch:{}\\n\".format(value, index+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with BOSSbase 1.01 WOW y PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the README, there is a link to download the databases we use for the work. There is BOSSbase 1.01, cover images and stego. The steganographic algorithms used in the paper are WOW and S-UNIWARD, with a payload of 0.4bpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose any of the five image normalizations and train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-12,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*20-12\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*20-12\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*20-12\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0\n",
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "print(X_valid.min())\n",
    "print(X_valid.max())\n",
    "print(X_test.min())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*2-1\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*2-1\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*2-1\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0.5\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0.5\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0.5\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN name and algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log_base = './logs/SR-Net/WOW'\n",
    "path_img_base = './image/SR-Net/WOW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SR_Net() \n",
    "train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32, epochs=100, model_name=\"SR-Net.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SR_Net() \n",
    "PATH_trained_models = \"\"\n",
    "Final_Results_Test(model,PATH_trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and testing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics(history, AccTest, LossTest, PATH_trained_models, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models(AccTest, accuracyTRAIN, accuracyVALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Semillero",
   "language": "python",
   "name": "semillero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
