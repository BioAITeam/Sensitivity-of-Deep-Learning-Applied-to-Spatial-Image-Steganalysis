{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ye-Net SRM Filters Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Lambda, Layer, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Concatenate\n",
    "tf.keras.layers.Concatenate()\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.layers.core import Reshape\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import Input, Model\n",
    "from time import time\n",
    "import time as tm\n",
    "from keras.initializers import Constant, RandomNormal, glorot_normal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import  concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  30 SRM filters Normalization x [1/12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('SRM_Kernels.npy')*(1/12) \n",
    "biasSRM=numpy.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ye-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ye_Net(img_size=256):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(img_size,img_size,1), name=\"input_1\")\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    #Block 1\n",
    "    layers = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "    #layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    #layer1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 2\n",
    "    \n",
    "    #layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    \n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 3\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 4\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 5\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(32, (5,5), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 6\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    layers = tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\")(layers)\n",
    "    layers = tf.keras.layers.Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    #layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "    layers = tf.keras.layers.Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    #layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "    layers = tf.keras.layers.Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    predictions = tf.keras.layers.Dense(2,kernel_initializer='glorot_normal', activation=\"softmax\", name=\"output_1\",kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    #Model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    #Optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.95)#lrate\n",
    "    #Compilator\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print (\"Ye-net model 2 generated\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining different functions to work with the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch = 0, model_name=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/logs/\"+model_name+\"_\"+\"{}\".format(time())\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "    filepath = log_dir+\"/saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max')\n",
    "    model.reset_states()\n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                        callbacks=[tensorboard,checkpoint], \n",
    "                        batch_size=batch_size,validation_data=(X_valid, y_valid),initial_epoch=initial_epoch)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/Results/\"+model_name+\"/\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "      \n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['lr'])\n",
    "        plt.ylabel('Lr')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.grid('on')\n",
    "        plt.show()\n",
    "        '''\n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Test(model,PATH_trained_models):\n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in os.listdir(PATH_trained_models):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = model.evaluate(X_test, y_test,verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n') \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def plot_train_valid(model,PATH_trained_models,model_name):\n",
    "    acc_train=[]\n",
    "    acc_valid=[]\n",
    "    loss_train=[]\n",
    "    loss_valid=[]\n",
    "    for filename in tqdm(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = model.evaluate(X_train, y_train,verbose=0)\n",
    "            acc_train.append(accuracy)\n",
    "            loss_train.append(loss)\n",
    "            loss,accuracy = model.evaluate(X_valid, y_valid,verbose=0)\n",
    "            acc_valid.append(accuracy)\n",
    "            loss_valid.append(loss)\n",
    "\n",
    "    results_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/Results/\"+model_name+\"/\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(acc_train)\n",
    "        plt.plot(acc_valid)\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(loss_train)\n",
    "        plt.plot(loss_valid)\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['lr'])\n",
    "        plt.ylabel('Lr')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.grid('on')\n",
    "        plt.show()\n",
    "        '''\n",
    "    results={'acc_train':acc_train,'acc_valid':acc_valid,'loss_train':loss_train,'loss_valid':loss_valid}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with BOSSbase 1.01 WOW y PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the README, there is a link to download the databases we use for the work. There is BOSSbase 1.01, cover images and stego. The steganographic algorithms used in the paper are WOW and S-UNIWARD, with a payload of 0.4bpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose any of the five image normalizations and train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-12,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*20-12\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*20-12\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*20-12\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0\n",
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "print(X_valid.min())\n",
    "print(X_valid.max())\n",
    "print(X_test.min())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*2-1\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*2-1\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*2-1\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0.5\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0.5\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0.5\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN name and algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name=\"04WOW1\"\n",
    "m_name=\"YE_Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=64, epochs=EPOCHS, model_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "PATH_trained_models = \" \"\n",
    "Final_Results_Test(model,PATH_trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and testing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net()  \n",
    "PATH_trained_models = \"\"\n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "res=plot_train_valid(model,PATH_trained_models,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Semillero",
   "language": "python",
   "name": "semillero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
