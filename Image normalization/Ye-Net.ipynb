{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ye-Net Image Normalization TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage, signal\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import ntpath\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from keras import optimizers \n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from time import time\n",
    "import time as tm\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "import glob\n",
    "from skimage.util.shape import view_as_blocks\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 SRM filters for preprocessing and the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('SRM_Kernels.npy') \n",
    "biasSRM=numpy.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/guide/tpu\n",
    "#https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb\n",
    "#https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=_pQCOmISAQBu\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ye-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ye_Net(img_size=256):\n",
    "    #tf.keras.backend.clear_session()\n",
    "    \n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(img_size,img_size,1), name=\"input_1\")\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    #Block 1\n",
    "    layers = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "    \n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 2\n",
    "    \n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 3\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 4\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 5\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(32, (5,5), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 6\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    layers = tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\")(layers)\n",
    "    layers = tf.keras.layers.Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    predictions = tf.keras.layers.Dense(2,kernel_initializer='glorot_normal', activation=\"softmax\", name=\"output_1\",kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    #Model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    #Optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.95)#lrate\n",
    "    #Compilator\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print (\"Ye-net model 2 generated\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining different functions to work with the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Valid(PATH_trained_models):\n",
    "    global AccValid\n",
    "    global LossValid\n",
    "    AccValid = []\n",
    "    LossValid = [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "                 _model = Ye_Net()\n",
    "            _model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = _model.evaluate(X_valid, y_valid, verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n')\n",
    "\n",
    "            BandAccValid  = accuracy\n",
    "            BandLossValid = loss\n",
    "            AccValid.append(BandAccValid)    \n",
    "            LossValid.append(BandLossValid)  \n",
    "            \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    \n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Train(PATH_trained_models):\n",
    "    global AccTrain\n",
    "    global LossTrain\n",
    "    AccTrain = []\n",
    "    LossTrain = [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "                 _model = Ye_Net()\n",
    "            _model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = _model.evaluate(X_train, y_train, verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n')\n",
    "\n",
    "            BandAccTrain  = accuracy\n",
    "            BandLossTrain = loss\n",
    "            AccTrain.append(BandAccTrain)    \n",
    "            LossTrain.append(BandLossTrain)  \n",
    "            \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    \n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Test(PATH_trained_models):\n",
    "    global AccTest\n",
    "    global LossTest\n",
    "    AccTest = []\n",
    "    LossTest= [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "                 _model = Ye_Net()\n",
    "            _model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = _model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n')\n",
    "\n",
    "            BandAccTest  = accuracy\n",
    "            BandLossTest = loss\n",
    "            AccTest.append(BandAccTest)    \n",
    "            LossTest.append(BandLossTest)  \n",
    "            \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    \n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics(AccTest, AccTrain, AccValid, LossTest, LossTrain, LossValid, model_name, path_img_base):\n",
    "    if not os.path.exists(path_img_base+\"/\"+model_name):\n",
    "       os.makedirs(path_img_base+\"/\"+model_name)\n",
    "\n",
    "    with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "        model = Ye_Net()\n",
    "    \n",
    "    lossTEST,accuracyTEST   = model.evaluate(X_test, y_test,verbose=None)\n",
    "    lossTRAIN,accuracyTRAIN = model.evaluate(X_train, y_train,verbose=None)\n",
    "    lossVALID,accuracyVALID = model.evaluate(X_valid, y_valid,verbose=None)\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(AccTrain)],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyVALID]),np.array(AccValid)],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTEST]),np.array(AccTest)],axis=0)) #Test\n",
    "        plt.title('Accuracy Vs Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Accuracy_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Accuracy_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Accuracy_Ye_Net_'+model_name+'.pdf', format='pdf')     \n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(LossTrain)],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossVALID]),np.array(LossValid)],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossTEST]),np.array(LossTest)],axis=0)) #Test\n",
    "        plt.title('Loss Vs Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Loss_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Loss_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/'+model_name+'/Loss_Ye_Net_'+model_name+'.pdf', format='pdf') \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_models(AccTest,AccTrain,AccValid):\n",
    "    numbers=AccTest\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Test Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=AccTrain\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Train Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=AccValid\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Validation Accuracy {}, epoch:{}\\n\".format(value, index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTPU(path_model, epochs, model_Name):\n",
    "    global model_name\n",
    "    start_time = tm.time()\n",
    "    model_name = model_Name\n",
    "    path_log_base = path_model+'/'+model_Name\n",
    "    if not os.path.exists(path_log_base):\n",
    "        os.makedirs(path_log_base)\n",
    "\n",
    "    with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "         model = Ye_Net()\n",
    "\n",
    "    epoch_ = 1\n",
    "    for epoch in range(epochs):\n",
    "        epoch=epoch+1\n",
    "        print(\"epoch \",epoch)\n",
    "        model.fit(X_train,y_train,validation_data=(X_valid,y_valid), batch_size=128*2, epochs=epoch_, verbose=1) \n",
    "        model.save_weights(path_model+'/'+model_name+'/'+str(epoch).zfill(4)+'.hdf5', overwrite=True) \n",
    "\n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with BOSSbase 1.01 WOW y PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the README, there is a link to download the databases we use for the work. There is BOSSbase 1.01, cover images and stego. The steganographic algorithms used in the paper are WOW and S-UNIWARD, with a payload of 0.4bpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose any of the five image normalizations and train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-12,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*20-12\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*20-12\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*20-12\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0\n",
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "print(X_valid.min())\n",
    "print(X_valid.max())\n",
    "print(X_test.min())\n",
    "print(X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*2-1\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*2-1\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*2-1\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [-0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = (X_train-X_train.min())/(X_train.max()-X_train.min())*1-0.5\n",
    "X_valid = (X_valid-X_valid.min())/(X_valid.max()-X_valid.min())*1-0.5\n",
    "X_test = (X_test-X_test.min())/(X_test.max()-X_test.min())*1-0.5\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN name and algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"./WOW/logs\"\n",
    "path_img_base = \"./Image/WOW/images\"\n",
    "\n",
    "model_Name = \"Ye-Net...\"\n",
    "\n",
    "trainTPU(path_model=path_model, epochs=100, model_Name = \"Ye-Net...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results_Test(path_model+\"/\"+model_Name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results_Valid(path_model+\"/\"+model_Name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results_Train(path_model+\"/\"+model_Name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and testing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics(AccTest, AccTrain, AccValid, LossTest, LossTrain, LossValid, model_Name, path_img_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models(AccTest,AccTrain,AccValid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to train the algorithm with S-UNIWARD 0.4 bpp, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Semillero",
   "language": "python",
   "name": "semillero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
